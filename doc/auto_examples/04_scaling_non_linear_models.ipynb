{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Fitting scalable, non-linear models on data with dirty categories\n\nA very classic dilemma when training a machine learning model consists in\nchoosing between using a linear model or a non-linear one.\n\nLinear models are very well studied and understood. They are generally fast\nand easy to optimize, and generate interpretable results. However, for some\nproblems with a complex relationship between the input and the output, linear\nmodels reach their expressivity limit: whatever the number of samples the\ntraining set may have, past some point, its precision won't get any better.\n\nNon-linear models, however, tend to *scale* better with sample size: they are\nable to digest the information in the additional samples to get a better\nestimate of the link between the input and the output.\n\nNon-linear models form a very large model class. Among others, this class\nincludes:\n\n* Neural Networks\n* Tree-based methods such as Random Forests, and the very powerful Gradient\n  Boosting Machines [#xgboost]_\n* Kernel Methods.\n\nHowever, reaching the phase where the non-linear model outperforms the linear\none can be complicated. Indeed, a more complex models often means a longer\nfitting/tuning process:\n\n* Neural networks often need extended model tuning time, in order to\n  achieve good optimization and network architecture.\n* Gradient Boosting Machines do not tend to scale extremely well with\n  increasing sample size, as all the data needs to be loaded into the main\n  memory.\n* For kernel methods, parameter fitting requires the inversion of a gram matrix\n  of size $n \\times n$ ($n$ being the number of samples), yielding\n  a quadratic dependency (with $n$) in the final computation time.\n\n\nIn order to get the best out of a non-linear model, one has to **make it\nscalable**. For kernel methods, approximation algorithms that drop the\nquadratic dependency with the sample size while ensuring almost the\nsame model capacity exist.\n\nIn this example, you will learn how to:\n    1. Build a ML pipeline that uses a kernel method.\n    2. Make this pipeline scalable, by using online algorithms and dimension\n       reduction methods.\n\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>This example assumes the reader is familiar with similarity encoding and\n   its use-cases.\n\n   * For an introduction to dirty categories, see `this example<sphx_glr_auto_examples_01_investigating_dirty_categories.py>`.\n   * To learn with dirty categories using the SimilarityEncoder, see `this example<sphx_glr_auto_examples_02_fit_predict_predict_employee_salaries.py>`.</p></div>\n\n\n.. |NYS| replace:: :class:`Nystroem <sklearn.kernel_approximation.Nystroem>`\n\n.. |NYS_EXAMPLE|\n    replace:: `scikit-learn documentation <nystroem_kernel_approx>`\n\n.. |RBF|\n    replace:: :class:`~sklearn.kernel_approximation.RBFSampler`\n\n.. |SVC|\n    replace:: :class:`SupportVectorClassifier <sklearn.svm.SVC>`\n\n.. |SE| replace:: :class:`~dirty_cat.SimilarityEncoder`\n\n.. |SGDClassifier| replace::\n    :class:`~sklearn.linear_model.SGDClassifier`\n\n.. |APS| replace::\n    :func:`~sklearn.metrics.average_precision_score`\n\n.. |OneHotEncoder| replace::\n    :class:`~sklearn.preprocessing.OneHotEncoder`\n\n.. |ColumnTransformer| replace::\n    :class:`~sklearn.compose.ColumnTransformer`\n\n.. |LabelEncoder| replace::\n    :class:`~sklearn.preprocessing.LabelEncoder`\n\n.. |SGDClassifier_partialfit| replace::\n    :meth:`~sklearn.linear_model.SGDClassifier.partial_fit`\n\n.. |Pipeline| replace::\n    :class:`~sklearn.pipeline.Pipeline`\n\n.. |pd_read_csv| replace::\n    :func:`pandas.read_csv`\n\n.. |sklearn| replace::\n    :std:doc:`scikit-learn <sklearn:index>`\n\n.. |transform| replace::\n    :std:term:`transform <sklearn:transform>`\n\n.. |fit| replace::\n    :std:term:`fit <sklearn:fit>`\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training a simple pipeline\nThe data that the model will fit is the :code:`drug_directory` dataset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from dirty_cat.datasets import fetch_drug_directory\n\n# We'll only gather the information, and not load the dataset in memory for now\ndrug_directory = fetch_drug_directory(load_dataframe=False)\nprint(drug_directory.description)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. topic:: Problem Setting\n\n   We set the goal of our machine learning problem as follows:\n\n   .. centered::\n      **predict the type of a drug given its composition.**\n\n\nThe :code:`NONPROPRIETARYNAME` column, is composed of text observations with\ndescribing each drug's composition. The :code:`PRODUCTTYPENAME` column\nconsists of categorical values: therefore, our problem is a classification\nproblem. You can have a glimpse of the values here:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n\ndf = pd.read_csv(\n    drug_directory.path,\n    quotechar=\"'\",\n    escapechar='\\\\',\n    nrows=10,\n).astype(str)\n# A simpler syntax we could use:\n# df = pd.read_csv(drug_directory.path, **drug_directory.read_csv_kwargs, nrows=10).astype(str)\nprint(df[['NONPROPRIETARYNAME', 'PRODUCTTYPENAME']].head())\n# This will be useful further down in the example.\ncolumns_names = df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Estimators construction\nOur input is categorical, thus needs to be encoded. As observations often\nconsist in variations around a few concepts (for instance,\n:code:`'Amlodipine Besylate'` and\n:code:`'Amlodipine besylate and atorvastatin calcium'`\nhave one ingredient in common), we need an encoding able to\ncapture similarities between observations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from dirty_cat import SimilarityEncoder\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import OneHotEncoder\nsimilarity_encoder = SimilarityEncoder(similarity='ngram')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Two other columns are used to predict the output: ``DOSAGEFORMNAME`` and\n``ROUTENAME``. They are both categorical and can be encoded with a\n|OneHotEncoder|. We use a |ColumnTransformer| to stack the |OneHotEncoder|\nand the |SE|.  We can now choose a kernel method, for instance a |SVC|, to\nfit the encoded inputs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\n\ncolumn_transformer = make_column_transformer(\n    (similarity_encoder, ['NONPROPRIETARYNAME']),\n    (OneHotEncoder(handle_unknown='ignore'), ['DOSAGEFORMNAME', 'ROUTENAME']),\n    sparse_threshold=1)\n\n# The classifier and the ColumnTransformer are stacked into a Pipeline object\nclassifier = SVC(kernel='rbf', random_state=42, gamma=1)\nsteps = [('transformer', column_transformer), ('classifier', classifier)]\nmodel = Pipeline(steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading and Preprocessing\nLike in most machine learning setups, the data has to be split into 2\nexclusive parts:\n\n* One for model training.\n* One for model testing.\n\nFor this reason, we create a simple wrapper around |pd_read_csv|, that\nextracts the :code:`X`, and :code:`y` from the dataset.\n\n.. topic:: Note about class imbalance:\n\n   The :code:`y` labels are composed of 7 unique classes. However, ``HUMAN\n   OTC DRUG`` and ``HUMAN PRESCRIPTION DRUG`` represent around 97% of the\n   data, in a fairly balanced manner. The last 5 classes are much rarer.\n   Dealing with class imbalance is out of the scope of this example, so the\n   models will be trained on the first two classes only.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def preprocess(df, label_encoder):\n    df = df.loc[df['PRODUCTTYPENAME'].isin(\n        ['HUMAN OTC DRUG', 'HUMAN PRESCRIPTION DRUG'])]\n\n    df = df[['NONPROPRIETARYNAME', 'DOSAGEFORMNAME', 'ROUTENAME', 'PRODUCTTYPENAME']]\n    df = df.dropna()\n\n    X = df[['NONPROPRIETARYNAME', 'DOSAGEFORMNAME', 'ROUTENAME']]\n    y = df[['PRODUCTTYPENAME']].values\n\n    y_int = label_encoder.transform(np.squeeze(y))\n\n    return X, y_int\n\n\ndef get_X_y(**kwargs):\n    \"\"\"simple wrapper around pd.read_csv that extracts features and labels\n\n    Some systematic preprocessing is also carried out to avoid doing this\n    transformation repeatedly in the code.\n    \"\"\"\n    global label_encoder\n    df = pd.read_csv(drug_directory.path, **drug_directory.read_csv_kwargs, **kwargs)\n    return preprocess(df, label_encoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Classifier objects in |sklearn| often require :code:`y` to be integer labels.\nAdditionally, |APS| requires a binary version of the labels.  For these two\npurposes, we create:\n\n* a |LabelEncoder|, that we pre-fitted on the known :code:`y` classes\n* a |OneHotEncoder|, pre-fitted on the resulting integer labels.\n\nTheir |transform| methods can the be called at appropriate times.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\nlabel_encoder = LabelEncoder()\nlabel_encoder.fit(['HUMAN OTC DRUG', 'HUMAN PRESCRIPTION DRUG'])\n\none_hot_encoder = OneHotEncoder(categories=\"auto\", sparse=False)\none_hot_encoder.fit([[0], [1]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-danger\"><h4>Warning</h4><p>During the following training procedures of this example, we will assume\n   that the dataset was shuffled prior to its loading. As a reason, we can\n   take the first $n$ observations for the training set, the next\n   $m$ observations for the test set and so on. This may not be the\n   case for all datasets, so be careful before applying this code to your own\n   !</p></div>\n\n\nFinally, the :code:`X` and :code:`y` are loaded.\n\n\n.. topic:: Note: offsetting the test set\n\n   We create an offset to separate the training and the test set. The reason\n   for this, is that the online procedures of this example will consume far\n   more rows, but we still would like to compare accuracies with the same the\n   same test set, and not change it each time. Therefore, we \"reserve\" the\n   first 100000 rows for the training phase. The rest is made available to\n   the test set.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_set_size = 5000\ntest_set_size = 10000\noffset = 100000\n\nX_train, y_train = get_X_y(skiprows=1, names=columns_names,\n                           nrows=train_set_size)\n\nX_test, y_test = get_X_y(skiprows=offset, names=columns_names,\n                         nrows=test_set_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluating time and sample complexity\nLet's get an idea of model precision and performance depending on the number\nof the samples used in the train set.\nThe |Pipeline| is trained over different training set sizes. For this,\n:code:`X_train` and :code:`y_train` get sliced into subsets of increasing\nsize, while :code:`X_test` and :code:`y_test` do not change when the\nsample size varies.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import time\nfrom sklearn.metrics import average_precision_score\n\n# define the different train set sizes on which to evaluate the model\ntrain_set_sizes = [train_set_size // 10, train_set_size // 3, train_set_size]\n\ntrain_times_svc, test_scores_svc = [], []\n\nfor n in train_set_sizes:\n\n    t0 = time.perf_counter()\n    model.fit(X_train[:n], y_train[:n])\n    train_time = time.perf_counter() - t0\n\n    y_pred = model.predict(X_test)\n\n    y_pred_onehot = one_hot_encoder.transform(y_pred.reshape(-1, 1))\n    y_test_onehot = one_hot_encoder.transform(y_test.reshape(-1, 1))\n\n    test_score = average_precision_score(y_test_onehot, y_pred_onehot)\n\n    train_times_svc.append(train_time)\n    test_scores_svc.append(test_score)\n\n    msg = (\"using {:>5} samples: model fitting took {:.1f}s, test accuracy of \"\n           \"{:.3f}\")\n    print(msg.format(n, train_time, test_score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Increasing training size clearly improves model accuracy. However, the\ntraining time and the input size increase quadratically with the training set\nsize.  Indeed, kernel methods need to process an entire $n \\times n$\nmatrix at once. In order for this matrix to be loaded into memory, $n$\nhas to remain low: Using 30000 observations, the input is a $30000\n\\times 30000$ matrix.  If composed of 32-bit floats, its total size is around\n$30000^2 \\times 32 = 2.8 \\times 10^{10} \\text{bits} = 4\\text{GB}$\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reducing input dimension using kernel approximation methods.\n\nThe main scalability issues with kernels methods is the processing of a large\nsquare matrix. To understand where this matrix comes from, we need to delve a\nlittle bit deeper these methods internals.\n\n.. topic:: Kernel methods\n\n   Kernel methods address non-linear problems by leveraging similarities\n   between each pair of inputs. Using a similarity matrix to solve a machine\n   learning problem allows to catch complex, non-linear relationships within\n   the data.  But it requires inverting this matrix, which can be a\n   computational burden when the sample sizes increases.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Kernel approximation methods\nFrom what was said below, two criteria are limiting a kernel algorithm to\nscale:\n\n* It processes a matrix, whose size increases quadratically with the number\n  of samples.\n* During fitting time, this matrix is **inverted**, meaning it has to be\n  loaded into main memory.\n\nKernel approximation methods such as |RBF| or |NYS| [#nys_ref]_ try to\napproximate this similarity matrix, without actually creating it. By allowing\nthe program to not compute the perfect similarity matrix, the problem\ncomplexity becomes linear! Plus, the samples also do not need to be processed\nat once into main memory. We are not bound to use a |SVC| anymore, and can\ninstead use an online optimization that will process the input by batch.\n\n.. topic:: Online algorithms\n\n   An online algorithm [#online_ref]_ is an algorithm that treats its input\n   piece by piece in a serial fashion. An famous example is the stochastic\n   gradient descent [#sgd_ref]_, where an estimation the objective function's\n   gradient is computed on a batch of the data at each step.\n\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reducing the transformers dimensionality\nThere is one last scalability issue in our pipeline: the |SE| and the |RBF|\nboth implement the |fit| method. How to adapt those method to an online\nsetting, where the data is never loaded as a whole?\n\nA simple solution is to partially fit the |SE| and the |RBF| on a subset of\nthe data, prior to the online fitting step.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.kernel_approximation import RBFSampler\nn_out_encoder = 1000\nn_out_rbf = 5000\nn_samples_encoder = 10000\n\nX_encoder, _ = get_X_y(nrows=n_samples_encoder, names=columns_names)\n\nsimilarity_encoder = SimilarityEncoder(\n    similarity='ngram', categories='most_frequent', n_prototypes=n_out_encoder,\n    random_state=42, ngram_range=(2, 4))\n\n# Fit the rbf_sampler with the similarity matrix.\ncolumn_transformer = make_column_transformer(\n    (similarity_encoder, ['NONPROPRIETARYNAME']),\n    (OneHotEncoder(handle_unknown='ignore'), ['DOSAGEFORMNAME', 'ROUTENAME']),\n    sparse_threshold=1)\n\ntransformed_categories = column_transformer.fit_transform(X_encoder)\n\n# gamma is a parameter of the rbf function, that sets how fast the similarity\n# between two points should decrease as the distance between them rises. It\n# is data-specific, and needs to be chosen carefully, for example using\n# cross-validation.\nrbf_sampler = RBFSampler(\n    gamma=0.5, n_components=n_out_rbf, random_state=42)\nrbf_sampler.fit(transformed_categories)\n\n\ndef encode(X, y_int, one_hot_encoder, column_transformer, rbf_sampler):\n    X_sim_encoded = column_transformer.transform(X)\n\n    X_highdim = rbf_sampler.transform(X_sim_encoded.toarray())\n\n    y_onehot = one_hot_encoder.transform(y_int.reshape(-1, 1))\n\n    return X_highdim, y_onehot\n\n\n# The inputs and labels of the val and test sets have to be pre-processed the\n# same way the training set was processed:\nX_test_kernel_approx, y_true_test_onehot = encode(\n    X_test, y_test, one_hot_encoder, column_transformer, rbf_sampler)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Online training for out-of-memory data\nWe now have all the theoretical elements to create an non-linear, online\nkernel method.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import warnings\nfrom sklearn.linear_model import SGDClassifier\n\nonline_train_set_size = 100000\n# Filter warning on max_iter and tol\nwarnings.filterwarnings('ignore', module='sklearn.linear_model')\nsgd_classifier = SGDClassifier(\n    max_iter=1, tol=None, random_state=42, average=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now start the training, by looping over batches one by one. Note that\nonly one pass over the whole dataset is done. It may be worth doing several\npasses, but for very large sample sizes, the increase in test accuracy is\nlikely to be marginal.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batchsize = 1000\ntest_scores_rbf = []\ntrain_times_rbf = []\nonline_train_set_sizes = []\nt0 = time.perf_counter()\n\niter_csv = pd.read_csv(\n    drug_directory.path, nrows=online_train_set_size, chunksize=batchsize,\n    skiprows=1, names=columns_names, **drug_directory.read_csv_kwargs)\n\nfor batch_no, batch in enumerate(iter_csv):\n    X_batch, y_batch = preprocess(batch, label_encoder)\n    # skip iteration if batch is empty after preprocessing\n    if len(y_batch) == 0:\n        continue\n    X_batch_kernel_approx, y_batch_onehot = encode(\n        X_batch, y_batch, one_hot_encoder, column_transformer, rbf_sampler)\n\n    # make one pass of stochastic gradient descent over the batch.\n    sgd_classifier.partial_fit(\n        X_batch_kernel_approx, y_batch, classes=[0, 1])\n\n    # print train/test accuracy metrics every 5 batch\n    if (batch_no % 5) == 0:\n        message = \"batch {:>4} \".format(batch_no)\n        for origin, X, y_true_onehot in zip(\n                ('train', 'val'),\n                (X_batch_kernel_approx, X_test_kernel_approx),\n                (y_batch_onehot, y_true_test_onehot)):\n\n            y_pred = sgd_classifier.predict(X)\n\n            # preprocess correctly the labels and prediction to match\n            # average_precision_score expectations\n            y_pred_onehot = one_hot_encoder.transform(\n                y_pred.reshape(-1, 1))\n\n            score = average_precision_score(y_true_onehot, y_pred_onehot)\n            message += \"{} precision: {:.4f}  \".format(origin, score)\n            if origin == 'val':\n                test_scores_rbf.append(score)\n                train_times_rbf.append(time.perf_counter() - t0)\n                online_train_set_sizes.append((batch_no + 1)*batchsize)\n\n        print(message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So far, we fitted two kinds of models: a exact kernel algorithm, and an\napproximate, online one. Lets compare both the accuracies and the number of\nvisited samples for each model as we increase our time budget:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\nf, axs = plt.subplots(2, 1, sharex=True, figsize=(10, 7))\nax_score, ax_capacity = axs\n\nax_score.set_ylabel('score')\nax_capacity.set_ylabel('training set size')\nax_capacity.set_xlabel('time')\n\nax_score.plot(train_times_svc, test_scores_svc, 'b-', label='exact')\nax_score.plot(train_times_rbf, test_scores_rbf, 'r-', label='online')\n\nax_capacity.plot(train_times_svc, train_set_sizes, 'b-', label='exact')\nax_capacity.plot(train_times_rbf, online_train_set_sizes, 'r-', label='online')\nax_capacity.set_yscale('log')\n\nax_score.legend(\n    bbox_to_anchor=(0., 1.02, 1., .102),\n    loc=3, ncol=2, mode='expand',\n    borderaxespad=0.)\n\n# compare the two methods in their common time range\nax_score.set_xlim(0, min(train_times_svc[-1], train_times_rbf[-1]))\n\ntitle = \"\"\"Test set accuracy and number of samples visited\nsamples seen by the SGDClassifier\"\"\"\nf.suptitle(title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This plot shows us that for the time budget, the online model will eventually\nprocess more samples, be faster and reach a far higher test accuracy that the\nnon-online, exact kernel method.\n\nOur online model also outperforms online **linear** models (for instance,\n|SE| + |SGDClassifier|). We did not fit two online models here for simplicity\npurposes, but to train an online linear model, simply comment out the line in\nencode :code:`X_highdim = rbf_sampler.transform(X_sim_encoded.toarray())` and\nchange it for example with :code:`X_highdim = X_sim_encoded`.\n\nIn particular, this hierarchy between the linear model and the non-linear\none shows that there were some significant non-linear relationships\nbetween the input and the output. By scaling a kernel method, we successfully\ntook this non-linearity into account in our model, which was a far from\ntrivial task at the beginning of this example!\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. rubric:: Footnotes\n.. [#xgboost] `Slides on gradient boosting by Tianqi Chen, the founder of XGBoost <https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf>`_\n.. [#online_ref] `Wikipedia article on online algorithms <https://en.wikipedia.org/wiki/Online_algorithm>`_\n.. [#sgd_ref] `Leon Bouttou's article on stochastic gradient descent <http://khalilghorbal.info/assets/spa/papers/ML_GradDescent.pdf>`_\n.. [#nys_ref] |NYS_EXAMPLE|\n.. [#dual_ref] `Introduction to duality <https://pdfs.semanticscholar.org/0373/e7289a1978108d6455218160a529c85842c0.pdf>`_\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}