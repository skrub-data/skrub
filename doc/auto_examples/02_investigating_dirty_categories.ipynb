{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Investigating dirty categories\n\nWhat are dirty categorical variables and how can a good encoding help\nwith statistical learning.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What do we mean by dirty categories?\n\nLet's look at a dataset called employee salaries:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from dirty_cat import datasets\n\nemployee_salaries = datasets.fetch_employee_salaries()\nprint(employee_salaries.description)\ndata = employee_salaries.X\nprint(data.head(n=5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is how many unique entries there is per column\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(data.nunique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see, some entries have many different unique values:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(data['employee_position_title'].value_counts().sort_index())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These different entries are often variations on the same entities:\nthere are 3 kinds of Accountant/Auditor.\n\nSuch variations will break traditional categorical encoding methods:\n\n* Using simple one-hot encoding will create orthogonal features,\n  whereas it is clear that those 3 terms have a lot in common.\n\n* If we wanted to use word embedding methods such as word2vec,\n  we would have to go through a cleaning phase: those algorithms\n  are not trained to work on data such as 'Accountant/Auditor I'.\n  However, this can be error prone and time consuming.\n\nThe problem becomes easier if we can capture relationships between\nentries.\n\nTo simplify understanding, we will focus on the column describing the\nemployee's position title:\ndata\nvalues = data[['employee_position_title', 'gender']] + employee_salaries.y\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "values = data[['employee_position_title', 'gender']]\nvalues.insert(0, 'current_annual_salary', employee_salaries.y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## String similarity between entries\n\nThat's where our encoders get into play. In order to robustly\nembed dirty semantic data, the SimilarityEncoder creates a similarity\nmatrix based on the 3-gram structure of the data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sorted_values = values['employee_position_title'].sort_values().unique()\n\nfrom dirty_cat import SimilarityEncoder\n\nsimilarity_encoder = SimilarityEncoder(similarity='ngram')\ntransformed_values = similarity_encoder.fit_transform(\n    sorted_values.reshape(-1, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plotting the new representation using multi-dimensional scaling\n\nLet's now plot a couple points at random using a low-dimensional representation\nto get an intuition of what the similarity encoder is doing:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import MDS\n\nmds = MDS(dissimilarity='precomputed', n_init=10, random_state=42)\ntwo_dim_data = mds.fit_transform(\n    1 - transformed_values)  # transformed values lie\n# in the 0-1 range, so 1-transformed_value yields a positive dissimilarity matrix\nprint(two_dim_data.shape)\nprint(sorted_values.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We first quickly fit a KNN so that the plots does not get too busy:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nn_points = 5\nnp.random.seed(42)\nfrom sklearn.neighbors import NearestNeighbors\n\nrandom_points = np.random.choice(len(similarity_encoder.categories_[0]),\n                                 n_points, replace=False)\nnn = NearestNeighbors(n_neighbors=2).fit(transformed_values)\n_, indices_ = nn.kneighbors(transformed_values[random_points])\nindices = np.unique(indices_.squeeze())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we plot it, adding the categories in the scatter plot:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\nf, ax = plt.subplots()\nax.scatter(x=two_dim_data[indices, 0], y=two_dim_data[indices, 1])\n# adding the legend\nfor x in indices:\n    ax.text(x=two_dim_data[x, 0], y=two_dim_data[x, 1], s=sorted_values[x],\n            fontsize=8)\nax.set_title(\n    'multi-dimensional-scaling representation using a 3gram similarity matrix')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Heatmap of the similarity matrix\n\nWe can also plot the distance matrix for those observations:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "f2, ax2 = plt.subplots(figsize=(6, 6))\ncax2 = ax2.matshow(transformed_values[indices, :][:, indices])\nax2.set_yticks(np.arange(len(indices)))\nax2.set_xticks(np.arange(len(indices)))\nax2.set_yticklabels(sorted_values[indices], rotation='30')\nax2.set_xticklabels(sorted_values[indices], rotation='60', ha='right')\nax2.xaxis.tick_bottom()\nax2.set_title('Similarities across categories')\nf2.colorbar(cax2)\nf2.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As shown in the previous plot, we see that the nearest neighbor of\n\"Communication Equipment Technician\"\nis \"telecommunication technician\", although it is also\nvery close to senior \"supply technician\": therefore, we grasp the\n\"communication\" part (not initially present in the category as a unique word)\nas well as the technician part of this category.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Encoding categorical data using SimilarityEncoder\n\nA typical data-science workflow uses one-hot encoding to represent\ncategories.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n\n# encoding simply a subset of the observations\nn_obs = 20\nemployee_position_titles = values['employee_position_title'].head(\n    n_obs).to_frame()\ncategorical_encoder = OneHotEncoder(sparse=False)\none_hot_encoded = categorical_encoder.fit_transform(employee_position_titles)\nf3, ax3 = plt.subplots(figsize=(6, 6))\nax3.matshow(one_hot_encoded)\nax3.set_title('Employee Position Title values, one-hot encoded')\nax3.axis('off')\nf3.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The corresponding is very sparse\n\nSimilarityEncoder can be used to replace one-hot encoding capturing the\nsimilarities:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "f4, ax4 = plt.subplots(figsize=(6, 6))\nsimilarity_encoded = similarity_encoder.fit_transform(employee_position_titles)\nax4.matshow(similarity_encoded)\nax4.set_title('Employee Position Title values, similarity encoded')\nax4.axis('off')\nf4.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Other examples in the dirty_cat documentation show how\nsimilarity encoding impacts prediction performance.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}